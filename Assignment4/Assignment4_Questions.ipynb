{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "The goal of this assignment is to use the knowledge gained in the course to develop an end-to-end SNN for MNIST digits classification trained using state-of-the-art gradient-descent algorithm. Along the way, you will also learn the basics of developing any machine learning application- how to handle data using data loaders; defining and optimizing loss; evaluating an algorithm using validation set; speeding up training using GPUs. You will also learn the basics of programming using PyTorch which is by far the most widely used library for machine learning research- being used in applications such as autonomous driving, robotic control, cancer research, and much more!\n",
    "\n",
    "Let's import all the libraries required for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, sampler\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Limitation of backprop for SNN\n",
    "\n",
    "## 1a. \n",
    "Sketch out the algorithm for training an SNN using backpropagation. Your algorithm should describe when and how the weights are updated. What is the main limitation of using backpropagation for training an SNN? Describe a solution to resolve the limitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a\n",
    "From our previous assignment, we established that Hebbian learning is a type of unsupervised learning that can best be summarized by the idea \"neurons that fire together wire together\". In terms of biological plausability, this means that the connection between neurons are stronger when activated _together_, or weaker otherwise. We use _linear_ Hebbian learning so that the synaptic weight between two neurons increases _proportionally_ to the correlation between the pre-synaptic and post-synaptic activities.\n",
    "\n",
    "We can add onto these ideas to sketch out the following generic algorithm for training a Spiking Neural Network (SNN) using backpropagation. Note that our algorithm uses Leaky Integrate-and-Fire (LIF) neurons for simplicity:\n",
    "\n",
    "1. **Encoding:** We encode the input layer of the SNN into spike trains and initialize the weights of the network randomly. This encoding can be rate-based or temporal-based.\n",
    "\n",
    "2. **Forward Propagation:** We propagate spikes from the input layer to the output layer, computing the membrane potentials and spike times for each neuron. We then generate output spikes and compute the network's output.\n",
    "\n",
    "3. **Error Loss:** We calculate the error between the network's output spikes and the target spike train using a suitable loss function.\n",
    "\n",
    "4. **Backward Propagation:** We propagate the error backward through the network. We compute gradients of the loss with respect to the network's weights using backpropagation through time.\n",
    "   \n",
    "5. **Weight Update:** We update the weights using the computed gradients and a gradient descent optimization algorithm, such as Stochastic Gradient Descent (SGD), and some learning rate. When it comes to supervised learning, it is also best to apply regularization to prevent overfitting.\n",
    "\n",
    "6. **Repeat:** Repeat from **Forward Propagation** until convergence or till the end of the duration of some epochs.\n",
    "\n",
    "The main limitation of using backpropagation for training SNNs is the non-differentiability of spike events. This discontinuity makes it challenging to compute gradients accurately and efficiently, leading to difficulties in training SNNs effectively.\n",
    "\n",
    "One way to resolve this limitation is to consider spike events as differentiable signals, but the non-differentiable spikes are considered 'noise'. That way, we can obtain smoother gradients and work directly on spike signals. These techniques enable effective training of SNNs by converting spike-based information into a form suitable for gradient-based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "In this exercise, we will implement the solution for overcoming backprop limitation for SNN. First the preliminaries: In PyTorch, arrays are called tensors. Gradients are computed automatically using the automatic differentiation package (autograd). The main elements of an autograd function are the forward and backward functions. The forward function simply performs the forward pass, i.e. computing output tensors from the input tensors. The backward function receives the gradient of the output tensors w.r.t. some scalar value, and computes the gradient of the input tensors w.r.t. the same scalar value. \n",
    "\n",
    "Below, we define the autograd function class for pseudo-gradient using the rectangular function. Most of the implementation is already written for you. Your task is to fill two key components- i) in the forward function, write the implementation for generating spike outputs from the inputs; ii) in the backward function, write the implementation for computing the gradient of the spike using rectangular psuedo-grad function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoSpikeRect(torch.autograd.Function):\n",
    "    \"\"\" Rectangular Pseudo-grad function \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, vth, grad_win, grad_amp):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input (Torch Tensor): Input tensor containing voltages of neurons in a layer\n",
    "            vth (Float): Voltage threshold for spiking \n",
    "            grad_win (Float): Window for computing pseudogradient\n",
    "            grad_amp (Float): Amplification factor for the gradients\n",
    "        \n",
    "        Returns:\n",
    "            output (Torch Tensor): Generated spikes for the input\n",
    "        \n",
    "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Saving variables for backward pass. Nothing to do here.\n",
    "        ctx.save_for_backward(input)\n",
    "        ctx.vth = vth\n",
    "        ctx.grad_win = grad_win\n",
    "        ctx.grad_amp = grad_amp\n",
    "        \n",
    "        # Compute output from the input. No loops.\n",
    "        # Hint: Use Pytorch \"greater than\" function.\n",
    "        output = torch.gt(input, vth).float()\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            grad_output (Torch Tensor): Gradient of the output\n",
    "        \n",
    "        Returns:\n",
    "            grad (Torch Tensor): Gradient of the input\n",
    "        \n",
    "        Write the operation for computing the output spikes from the input. The operation should be vectorized, i.e. no loops. \n",
    "        \"\"\"\n",
    "        \n",
    "        # Retrieving variables from forward pass. Nothing to do here.\n",
    "        input, = ctx.saved_tensors\n",
    "        vth = ctx.vth\n",
    "        grad_win = ctx.grad_win\n",
    "        grad_amp = ctx.grad_amp\n",
    "        grad_input = grad_output.clone()\n",
    "        \n",
    "        # Compute the gradient of the input using rectangular pseudograd function.\n",
    "        # https://arxiv.org/pdf/2003.01157\n",
    "        spike_pseudo_grad = torch.lt(torch.abs(torch.sub(input, vth)), grad_win)\n",
    "        \n",
    "        # Multiplying by gradient amplifier. Nothing to do here.\n",
    "        grad = grad_amp * grad_input * spike_pseudo_grad.float()\n",
    "        return grad, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Creating a Layer\n",
    "\n",
    "In this exercise, we will create the class definition for a layer of LIF neurons (similar to the implementation in Assignment 2).\n",
    "\n",
    "Below is the class definition of an LIF neuron layer. Your task is to write the operation for integrating the presynaptic spikes into voltage, and then transforming to spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearIFCell(nn.Module):\n",
    "    \"\"\" Leaky Integrate-and-fire neuron layer\"\"\"\n",
    "\n",
    "    def __init__(self, psp_func, pseudo_grad_ops, param):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_func (Torch Function): Pre-synaptic function\n",
    "            pseudo_grad_ops (Torch Function): Pseudo-grad function\n",
    "            param (tuple): Cell parameters (Voltage Threshold, gradient window, gradient amplitude)\n",
    "        \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        super(LinearIFCell, self).__init__()\n",
    "        self.psp_func = psp_func\n",
    "        self.pseudo_grad_ops = pseudo_grad_ops\n",
    "        self.vdecay, self.vth, self.grad_win, self.grad_amp = param\n",
    "\n",
    "    def forward(self, input_data, state):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            input_data (Tensor): input spike from pre-synaptic neurons\n",
    "            state (tuple): output spike of last timestep and voltage of last timestep\n",
    "        Returns:\n",
    "            output: output spike\n",
    "            state: updated neuron states\n",
    "        \n",
    "        Write the operation for integrating the presynaptic spikes into voltage.\n",
    "        \"\"\"\n",
    "        pre_spike, pre_volt = state\n",
    "        \n",
    "        # Compute the voltage from the presynaptic inputs. This should be a vectorized operation. No loops.\n",
    "        # Assignment2: self.volt = self.volt * self.vdecay + psp_input\n",
    "        volt = pre_volt * self.vdecay + self.psp_func.forward(input_data)\n",
    "        \n",
    "        # Compute the spike output by using the pseudo_grad_ops function. This should be a vectorized operation. No loops.\n",
    "        output = self.pseudo_grad_ops(volt, self.vth, self.grad_win, self.grad_amp)\n",
    "        \n",
    "        return output, (output, volt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Creating a Network \n",
    "\n",
    "## 3a.\n",
    "We will now create a one-layer SNN using the class definitions above. Preliminaries: In Assignment 2, the psp was computed using numpy matrix multiplication of weights and inputs. In PyTorch, nn.Linear() achieves the same. You can find the documentation here: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html. We will use this class to serve as our psp function required for creating a layer according to the implementation in Q2. \n",
    "\n",
    "Below is the class definition of a network. Your task is to fill in the required components in the init and forward functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleHiddenLayerSNN(nn.Module):\n",
    "    \"\"\" SNN with single hidden layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden layer dimension\n",
    "            param_dict (dict): neuron parameter dictionary for each layer (Voltage Threshold, gradient window, gradient amplitude)\n",
    "        \n",
    "        Create hidden and output layers using implementation of the layer in Q2. and using nn.Linear as the psp function. \n",
    "        \"\"\"\n",
    "        super(SingleHiddenLayerSNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        pseudo_grad_ops = PseudoSpikeRect.apply\n",
    "        \n",
    "        # Create the hidden layer.\n",
    "        # Assume that the hidden layer neuron parameters are in param_dict['hid_layer']. Set bias=False for nn.Linear.  \n",
    "        self.hidden_cell = LinearIFCell(nn.Linear(input_dim, hidden_dim, bias=False), pseudo_grad_ops, param_dict['hid_layer'])\n",
    "        \n",
    "        # Create the output layer.\n",
    "        # Output layer params are in param_dict['out_layer']. Set bias=False for nn.Linear.   \n",
    "        self.output_cell = LinearIFCell(nn.Linear(hidden_dim, output_dim, bias=False), pseudo_grad_ops, param_dict['out_layer'])\n",
    "\n",
    "    def forward(self, spike_data, init_states_dict, batch_size, spike_ts):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            spike_data (Tensor): spike data input (batch_size, input_dim, spike_ts)\n",
    "            init_states_dict (dict): initial states for each layer- 'hid_layer' for hidden layer; 'out_layer' for output layer. \n",
    "            batch_size (int): batch size\n",
    "            spike_ts (int): spike timesteps\n",
    "        Returns:\n",
    "            output: number of spikes of output layer\n",
    "        \n",
    "        Write the operations for propagating the input through the network and computing the spike outputs. \n",
    "        \"\"\"\n",
    "        hidden_state, out_state = init_states_dict['hid_layer'], init_states_dict['out_layer']\n",
    "        spike_data_flatten = spike_data.view(batch_size, self.input_dim, spike_ts)\n",
    "        output_list = [] # List to store the output at each timestep.\n",
    "        \n",
    "        for tt in range(spike_ts):\n",
    "            # Retrieve the input at time tt.\n",
    "            input_spikes = spike_data_flatten[:, :, tt]\n",
    "            \n",
    "            # Propagate through the hidden layer.\n",
    "            hidden_spikes, hidden_state = self.hidden_cell.forward(input_spikes, hidden_state)\n",
    "            \n",
    "            # Propagate through the output layer.\n",
    "            output_spikes, out_state = self.output_cell.forward(hidden_spikes, out_state)\n",
    "            \n",
    "            # Append output spikes to output list.\n",
    "            output_list.append(output_spikes)\n",
    "        \n",
    "        # Sum the outputs to compute spike count for each output neuron. No loops.\n",
    "        # Hint: Torch.stack and Torch.sum might be useful here.\n",
    "        output = torch.sum(torch.stack(output_list), dim=0)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. \n",
    "Next, we need a Wrapper class that: i) Initializes the parameters required for creating the SNN class object; ii) creates the SNN class objects using the initial parameters; iii) Computes the SNN output and returns it. The class is already written for you. You do not need to do anything here. Just understand the implementation so that you can use it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapSNN(nn.Module):\n",
    "    \"\"\" Wrapper of SNN \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, param_dict, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_dim (int): input dimension\n",
    "            output_dim (int): output dimension\n",
    "            hidden_dim (int): hidden layer dimension\n",
    "            param_dict (dict): neuron parameter dictionary\n",
    "            device (device): device\n",
    "        \"\"\"\n",
    "        super(WrapSNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        self.snn = SingleHiddenLayerSNN(input_dim, output_dim, hidden_dim, param_dict)\n",
    "\n",
    "    def forward(self, spike_data):\n",
    "        \"\"\"\n",
    "        Forward function\n",
    "        Args:\n",
    "            spike_data (Tensor): spike data input\n",
    "        Returns:\n",
    "            output: number of spikes of output layer\n",
    "        \"\"\"\n",
    "        batch_size = spike_data.shape[0]\n",
    "        spike_ts = spike_data.shape[-1]\n",
    "        init_states_dict = {}\n",
    "        # Hidden layer.\n",
    "        hidden_volt = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        hidden_spike = torch.zeros(batch_size, self.hidden_dim, device=self.device)\n",
    "        init_states_dict['hid_layer'] = (hidden_spike, hidden_volt)\n",
    "        # Output layer.\n",
    "        out_volt = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
    "        out_spike = torch.zeros(batch_size, self.output_dim, device=self.device)\n",
    "        init_states_dict['out_layer'] = (out_spike, out_volt)\n",
    "        # SNN.\n",
    "        output = self.snn(spike_data, init_states_dict, batch_size, spike_ts)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Encoding MNIST into spikes\n",
    "Following is the function that converts an MNIST image into spikes. You have already implemented it using Numpy in Assignment 2. The implementation remains the same- except that we will now use Torch tensors instead of numpy arrays. Fill in the components to convert a batch of torch tensors into spikes. Since the goal is to learn writing optimized code using PyTorch, you are supposed to do this without any loops. Use vector operations instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_2_event_img(image, device, spike_ts):\n",
    "    \"\"\"\n",
    "    Transform image to event image\n",
    "    Args:\n",
    "        image (Tensor): image\n",
    "        device (device): device (can be either CPU or GPU)\n",
    "        spike_ts (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image\n",
    "    \"\"\"\n",
    "    batch_size = image.shape[0]\n",
    "    channel_size = image.shape[1]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.view(batch_size, channel_size, image_size, image_size, 1)\n",
    "    \n",
    "    # Create a random image of shape batch_size x channel_size x image_size x image_size x spike_ts.\n",
    "    # Hint: Torch rand function might be useful here.\n",
    "    # Remember to put the random image on the device specified in the function argument.\n",
    "    random_image = torch.rand(batch_size, channel_size, image_size, image_size, spike_ts, device=device)\n",
    "    \n",
    "    # Generate event image using image and random image.\n",
    "    event_image = torch.gt(image, random_image).float()\n",
    "\n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Training the SNN\n",
    "\n",
    "In this exercise, we will write the function to train an SNN using spatiotemporal backprop (stbp). A typical training loop works as follows:\n",
    "\n",
    "1. Split the dataset into train and test. The network is trained on all the batches in the train dataset, and then validated on the test dataset. This gives us an idea of how well the network generalizes to unseen data. \n",
    "\n",
    "2. A criterion is defined to compute the loss. For classification tasks, this is generally Cross Entropy (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "\n",
    "3. The network is initialized with random weights. \n",
    "\n",
    "4. Typically, the training is done on mini-batches of train data. This means that at every training instance, the network receives batches of training data where each batch contains small number of samples. The loss and the gradients required for updating the weights are averaged over these batches. \n",
    "\n",
    "5. Within every training iteration, first we retrieve the batches of data. We compute the network prediction. Then we compute the loss by comparing the network prediction against the true labels. Then the gradient of the loss with respect to all the network weights is computed. This gradient is then used to update the weights in the network.  \n",
    "\n",
    "Thankfully, PyTorch provides APIs to automate most of the above steps. Below is the function training an SNN that implements the algorithm presented above using PyTorch. Your task is to fill the components. Refer to the comments for hints on what PyTorch functions to use.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stbp_snn_training(network, spike_ts, device, batch_size=128, test_batch_size=256, epoch=100):\n",
    "    \"\"\"\n",
    "    STBP SNN training\n",
    "    Args:\n",
    "        network (SNN): STBP learning SNN\n",
    "        spike_ts (int): spike timestep\n",
    "        device (device): device\n",
    "        batch_size (int): batch size for training\n",
    "        test_batch_size (int): batch size for testing\n",
    "        epoch (int): number of epochs\n",
    "    Returns:\n",
    "        train_loss_list: list of training loss for each epoch\n",
    "        test_accuracy_list: list of test accuracy for each epoch\n",
    "    \"\"\"\n",
    "    \n",
    "    # Creating folder where MNIST data is saved. Loading the MNIST dataset. This code is complete. Do not touch it.\n",
    "    try:\n",
    "        os.mkdir(\"./data\")\n",
    "        print(\"Directory data Created\")\n",
    "    except FileExistsError:\n",
    "        print(\"Directory data already exists\")\n",
    "    \n",
    "    data_path = './data/'\n",
    "    train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=True,\n",
    "                                               transform=transforms.ToTensor())\n",
    "    test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=True,\n",
    "                                              transform=transforms.ToTensor())\n",
    "\n",
    "    # Train and test dataloader.\n",
    "    \n",
    "    # Given the train and test datasets, we need to create dataloaders to load the datasets in the right format.\n",
    "    # You can read about PyTorch Dataset and Dataloaders here: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html.\n",
    "    # For now, this part is complete and you do not need to do anything here.\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  shuffle=False, num_workers=4)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=test_batch_size,\n",
    "                                 shuffle=False, num_workers=4)\n",
    "    \n",
    "    # Next, we need to define a criteria for computing the loss.\n",
    "    # Refer to https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html on how to define the cross entropy loss.\n",
    "    # Note that you just need to define the loss here (and not compute it).\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Define an optimizer that will perform the weight updates.\n",
    "    # You can find more about the optimizers in PyTorch here: https://pytorch.org/docs/stable/optim.html.\n",
    "    # Taking the help of the documentation above, create an optimizer for stochastic gradient descent (SGD).\n",
    "    optimizer = torch.optim.SGD(network.parameters(), lr=0.01)\n",
    "\n",
    "    # List for saving loss and accuracy.\n",
    "    train_loss_list, test_accuracy_list = [], []\n",
    "    test_num = len(test_dataset)\n",
    "    \n",
    "    # Start training.\n",
    "    \n",
    "    # Put the network on the device (typically GPU but can also be cpu).\n",
    "    network.to(device)\n",
    "    \n",
    "    # Loop for the epochs.\n",
    "    for ee in range(epoch):\n",
    "        # Keep track of running loss.\n",
    "        running_loss = 0.0\n",
    "        running_batch_num = 0\n",
    "        train_start = time.time()\n",
    "        \n",
    "        # Iterate over the training data in train dataloader.\n",
    "        for data in train_dataloader:\n",
    "            # Retrieve the images and labels from data.\n",
    "            images, labels = data\n",
    "            \n",
    "            # Put the images and labels on the device.\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Convert images to event images.\n",
    "            event_images = img_2_event_img(images, device, spike_ts)\n",
    "            \n",
    "            # Before we backprop, we need to set the gradients for each tensor to zero.\n",
    "            # This is done using the zero_grad function in Pytorch.\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute the network output for the event images.\n",
    "            outputs = network(event_images)\n",
    "            \n",
    "            # Compute the loss using the criterion defined previously.\n",
    "            # Store in a variable called loss.\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backpropagate the loss through the network.\n",
    "            # Use Pytorch backward() function: https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html.\n",
    "            loss.backward()\n",
    "            \n",
    "            # Update the network weights by taking an optimizer 'step'. \n",
    "            # You can learn how to do that here: https://pytorch.org/docs/stable/optim.html#taking-an-optimization-step.\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Updating tracking variables. Nothing to do here.\n",
    "            running_loss += loss.item()\n",
    "            running_batch_num += 1\n",
    "        \n",
    "        train_end = time.time()\n",
    "        train_loss_list.append(running_loss / running_batch_num)\n",
    "        print(\"Epoch %d Training Loss %.4f\" % (ee, train_loss_list[-1]), end=\" \")\n",
    "        \n",
    "        \n",
    "        # This ends one training iteration.\n",
    "        # After every training iteration, we can evaluate how well the network does on data that it has not seen before.\n",
    "        # This step is called testing and is done on test dataset.\n",
    "        \n",
    "        # Counter to keep track of the number of correct predictions.\n",
    "        test_correct_num = 0\n",
    "        test_start = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in test_dataloader:\n",
    "                \n",
    "                # Retrieve the images and labels from test data.\n",
    "                images, labels = data\n",
    "                \n",
    "                # Put the images and labels on the device.\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                # Convert the image into event images.\n",
    "                event_images = img_2_event_img(images, device, spike_ts)\n",
    "                \n",
    "                # Compute the network predictions and store in a variable called outputs.\n",
    "                outputs = network(event_images)\n",
    "                \n",
    "                # Get the class label as the largest activation. This is complete. Nothing to do here.\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                \n",
    "                # Compare the network predictions against the true labels and update the counter for correct predictions. No loops.\n",
    "                test_correct_num += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Updating tracking variables. Nothing to do here.\n",
    "        test_end = time.time()\n",
    "        test_accuracy_list.append(test_correct_num / test_num)\n",
    "        print(\"Test Accuracy %.4f Training Time: %.1f Test Time: %.1f\" % (\n",
    "            test_accuracy_list[-1], train_end - train_start, test_end - test_start))\n",
    "    \n",
    "    # Return the loss and accuracies. Nothing to do here.\n",
    "    print(\"End Training\")\n",
    "    network.to('cpu')\n",
    "    return train_loss_list, test_accuracy_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have everything ready to train and test our SNN using backprop. All that is left to do is initialize the network with the right parameters and call the training function on it. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Directory data already exists\n",
      "Epoch 0 Training Loss 0.6199 Test Accuracy 0.8982 Training Time: 103.3 Test Time: 2.0\n",
      "Epoch 1 Training Loss 0.3026 Test Accuracy 0.9173 Training Time: 66.8 Test Time: 2.2\n",
      "Epoch 2 Training Loss 0.2358 Test Accuracy 0.9311 Training Time: 58.8 Test Time: 2.0\n",
      "Epoch 3 Training Loss 0.1964 Test Accuracy 0.9335 Training Time: 61.5 Test Time: 2.0\n",
      "Epoch 4 Training Loss 0.1733 Test Accuracy 0.9491 Training Time: 57.8 Test Time: 2.1\n",
      "Epoch 5 Training Loss 0.1521 Test Accuracy 0.9484 Training Time: 62.3 Test Time: 2.1\n",
      "Epoch 6 Training Loss 0.1379 Test Accuracy 0.9527 Training Time: 46.9 Test Time: 2.2\n",
      "Epoch 7 Training Loss 0.1241 Test Accuracy 0.9552 Training Time: 50.1 Test Time: 2.0\n",
      "Epoch 8 Training Loss 0.1125 Test Accuracy 0.9489 Training Time: 51.7 Test Time: 2.0\n",
      "Epoch 9 Training Loss 0.1074 Test Accuracy 0.9551 Training Time: 57.0 Test Time: 2.0\n",
      "Epoch 10 Training Loss 0.0964 Test Accuracy 0.9538 Training Time: 54.2 Test Time: 2.1\n",
      "Epoch 11 Training Loss 0.0925 Test Accuracy 0.9590 Training Time: 57.3 Test Time: 2.0\n",
      "Epoch 12 Training Loss 0.0877 Test Accuracy 0.9612 Training Time: 58.3 Test Time: 2.0\n",
      "Epoch 13 Training Loss 0.0793 Test Accuracy 0.9595 Training Time: 49.2 Test Time: 2.1\n",
      "Epoch 14 Training Loss 0.0774 Test Accuracy 0.9595 Training Time: 62.5 Test Time: 2.0\n",
      "Epoch 15 Training Loss 0.0737 Test Accuracy 0.9653 Training Time: 69.4 Test Time: 2.1\n",
      "Epoch 16 Training Loss 0.0669 Test Accuracy 0.9641 Training Time: 76.9 Test Time: 2.1\n",
      "Epoch 17 Training Loss 0.0619 Test Accuracy 0.9625 Training Time: 61.0 Test Time: 2.0\n",
      "Epoch 18 Training Loss 0.0614 Test Accuracy 0.9666 Training Time: 62.2 Test Time: 2.0\n",
      "Epoch 19 Training Loss 0.0554 Test Accuracy 0.9670 Training Time: 65.7 Test Time: 2.0\n",
      "Epoch 20 Training Loss 0.0546 Test Accuracy 0.9663 Training Time: 64.3 Test Time: 2.1\n",
      "Epoch 21 Training Loss 0.0534 Test Accuracy 0.9663 Training Time: 66.6 Test Time: 2.1\n",
      "Epoch 22 Training Loss 0.0501 Test Accuracy 0.9650 Training Time: 64.0 Test Time: 2.1\n",
      "Epoch 23 Training Loss 0.0470 Test Accuracy 0.9672 Training Time: 66.1 Test Time: 2.1\n",
      "Epoch 24 Training Loss 0.0457 Test Accuracy 0.9661 Training Time: 60.7 Test Time: 1.8\n",
      "Epoch 25 Training Loss 0.0459 Test Accuracy 0.9675 Training Time: 76.2 Test Time: 2.1\n",
      "Epoch 26 Training Loss 0.0444 Test Accuracy 0.9656 Training Time: 60.7 Test Time: 2.0\n",
      "Epoch 27 Training Loss 0.0409 Test Accuracy 0.9704 Training Time: 60.5 Test Time: 2.3\n",
      "Epoch 28 Training Loss 0.0380 Test Accuracy 0.9678 Training Time: 68.5 Test Time: 2.2\n",
      "Epoch 29 Training Loss 0.0383 Test Accuracy 0.9689 Training Time: 61.2 Test Time: 2.1\n",
      "End Training\n",
      "Train Loss:\n",
      " [0.6199168796335329, 0.3025931436588356, 0.23576788734922657, 0.19640362034164577, 0.1732551594401823, 0.1521129951770626, 0.13786960392495368, 0.12407888893161152, 0.11250419116247373, 0.10743473310952883, 0.09642115684110981, 0.09253911797834961, 0.08768666955008944, 0.07929483403973996, 0.07742926003937639, 0.07366514377105338, 0.06686425125494457, 0.06194598576538503, 0.06136968258722648, 0.05538490175831515, 0.05455137535171119, 0.053409385602119, 0.05007365451746388, 0.04698198590788053, 0.045682237906528436, 0.04594635369746622, 0.044413953132655365, 0.04092129226451291, 0.038029081848904744, 0.0383021073708279]\n",
      "Test Accuracy\n",
      ": [0.8982, 0.9173, 0.9311, 0.9335, 0.9491, 0.9484, 0.9527, 0.9552, 0.9489, 0.9551, 0.9538, 0.959, 0.9612, 0.9595, 0.9595, 0.9653, 0.9641, 0.9625, 0.9666, 0.967, 0.9663, 0.9663, 0.965, 0.9672, 0.9661, 0.9675, 0.9656, 0.9704, 0.9678, 0.9689]\n"
     ]
    }
   ],
   "source": [
    "# Define the device on which training will be performed.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache() # New--Not in initial assignment.\n",
    "print('Device:', device)\n",
    "\n",
    "# Define the input dimensions in a variable.\n",
    "input_dim = 784 # MNIST Image Size.\n",
    "\n",
    "# Define the output dimensions in a variable.\n",
    "output_dim = 10 # MNIST Class Size.\n",
    "\n",
    "# Define the hidden layer dimension in a variable.\n",
    "hidden_dim = 256\n",
    "\n",
    "# Create a dictionary of the neuron parameters for the hidden and output layer. The keys should be 'hid_layer' and 'out_layer'.\n",
    "# The values of the dictionary should be a list of the neuron parameters for each layer where the list elements are [vdecay, vth, grad_win, grad_amp]\n",
    "neuron_params = {\n",
    "    'hid_layer': [0.3, 0.3, 0.3, 1.0],\n",
    "    'out_layer': [0.3, 0.3, 0.3, 1.0]\n",
    "}\n",
    "\n",
    "# Define snn timesteps in a variable.\n",
    "snn_timesteps = 10 # Reduced to 10 because runtime was too long otherwise.\n",
    "\n",
    "# Create the SNN using the class definition in 3b and the arguments defined above.\n",
    "snn = WrapSNN(input_dim, output_dim, hidden_dim, neuron_params, device)\n",
    "\n",
    "# Define the following training parameters.\n",
    "\n",
    "# Batch size for training.\n",
    "train_batch_size = 128\n",
    "\n",
    "# Batch size for testing.\n",
    "test_batch_size = 256\n",
    "\n",
    "# Epochs.\n",
    "epochs = 30\n",
    "\n",
    "# Train the snn using the above arguments and the definition in Q5.\n",
    "train_loss_list, test_accuracy_list = stbp_snn_training(snn, snn_timesteps, device, train_batch_size, test_batch_size, epochs)\n",
    "print('Train Loss:\\n', train_loss_list)\n",
    "print('Test Accuracy\\n:', test_accuracy_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6: Concluding Remarks \n",
    "If you have been able to implement all the parts in Q.1.-Q.5., you can now feel proud of yourself for having learned the basics of a deep learning pipeline that powers all the modern AI applications. Of course, the exact implementation varies in the input and network design for more complex applications, but the pipeline remains the same. In fact, we went beyond the conventional deep learning, and implemented a spiking deep learning algorithm which is not a skill that many people possess. \n",
    "\n",
    "Now that you know the basic implementation, there are lots of directions which you can pursue if you are interested in this research area- how do the hyper-parameters such as the pseudo-gradient window affect the network training process? How much training data do you need to achieve good performance? How does the amount of training data vary with the complexity of the task? To what limits can you push the spike encoding, i.e. what is the minimum timesteps you need to achieve good performance? \n",
    "\n",
    "You do not need to answer these questions in this assignment but it is something to think about if you are interested. \n",
    "\n",
    "**As the concluding question of the course, can you describe what are the three key lessons you learned from the course and why is it important to learn them?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 6\n",
    "I thoroughly enjoyed this class! The content was informative, and the delivery was concise and clear. Moreover, the assignments were incredibly helpful, offering a great deal of guidance for implementing neural networks with a focus on biophysical properties.\n",
    "\n",
    "1. My first key lesson from this class was delving into the biophysical nature of _spiking_ neural networks. As a Computer Science major, my previous encounters with neural networks were often abstract and mechanical. We focused on input states, output states, and the flow of information between layers. Typically, the emphasis was on supervised learning, where understanding input-output relationships is more straightforward. However, what made this course stand out was the exploration of spiking neural networks and their biophysical underpinnings. With spiking neural networks, we were able to model the behavior of biological neurons more closely. This meant delving into concepts like membrane potential, spike generation, and synaptic plasticity. In fact, Assignment 1 was hefty, but it was also my favorite assignment. I felt like a neuroscientist!\n",
    "\n",
    "2. My second key lesson from this class was the fascinating capabilities of neurons. When studying motor neurons, it was mind-blowing to come to know that neurons encode sequences of movements for muscles, rather than individual muscles themselves. Considering the vast range of motion our bodies are capable of, this encoding mechanism is honestly remarkable. Understanding these finer details of neural function is crucial for practical implications. For instance, in neuroscience, understanding _how_ and _which_ neurons encode information can lead to insights into neurological disorders and potential treatments.\n",
    "\n",
    "3. My final key lesson from this class was learning about Hebbian learning. I quoted \"neurons that fire together, wire together\" a little too much for Assignment 3. I enjoyed learning about this particular form of unsupervised learning and how it leads to weight increments to detects correlation among inputs and outputs because it provided unique insights into associative learning and memory formation. Additionally, delving into Hebbian learning not only introduced fundamental principles of neural plasticity but also offered practical implications for artificial neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

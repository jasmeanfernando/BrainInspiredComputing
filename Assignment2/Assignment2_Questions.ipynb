{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "\n",
    "In Assignment 1, we studied how information is represented by a single spiking neuron. In this assignment, you will learn how to construct networks of spiking neurons for a given cognitive task, how to propagate information through a network, and understanding the intuition behind network design choices.Â \n",
    "\n",
    "Let's first import all the libraries required for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: From single neuron to network of neurons\n",
    "## 1a.\n",
    "What computational advantages do networks of neurons offer when compared against information processing by a single neuron? In other words, why do we need networks of neurons? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a.\n",
    "Single neuron models, such as the Hodgkin-Huxley (HH) and Leaky Integrate-And-Fire (LIF), offer computational advantages by allowing us to observe how action potentials (impulses) are generated and propagated in neurons. The Hodgkin-Huxley model is a mathematical representation that accurately captures the biophysical features of real neurons. By incorporating voltage-gated ion channels and considering their dynamics, this model provides a detailed description of how changes in membrane conductance lead to action potential generation. On the other hand, the Leaky Integrate-And-Fire model, while less detailed, is computationally efficient and easier to implement in simulations, particularly for neural networks, due to its simplicity.\n",
    "\n",
    "While single neuron models have their advantages, networks of neurons also offer distinct computational benefits. Networks of neurons are capable of handling more complex computations compared to single neurons. By connecting multiple neurons into networks, it becomes feasible to perform a wide array of information processing tasks. In fact, networks can encode and process information using patterns of activity distributed across many neurons. This distributed representation enables the network to capture complex relationships and features present in the input data. Additionally, neural networks can be used to implement training algorithms, such as softmax regression (something I *tried* to implement in my Intro to AI class!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Describe the algorithm for the information flow through a network of spiking leaky-integrate-and-fire (LIF) neurons. Specifically, trace out the steps required to compute network output from a given (continuous-valued) inputs. The algorithm should describe how continuous-valued inputs are fed to the SNN input layer, how the layer activations are computed, and how the output layer activity is decoded. Also, provide a diagrammatic overview of the algorithm to aid your explanation. You are free to assume any network size, and input and output dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b.\n",
    "We consider a basic feedforward spiking neural network with one input layer, one hidden layer, and one output layer. The steps involved in computing the network output from continuous-valued inputs are as follows:\n",
    "\n",
    "**Input Layer:**\n",
    "- Continuous-valued inputs are encoded into spike trains that represent the input signals to the network. This encoding can be achieved using techniques such as rate coding, where the firing rate of neurons in the input layer represents the *magnitude* of the input values, or temporal coding, where spike *timing* encodes the input values.\n",
    "\n",
    "**Hidden Layers:**\n",
    "- The encoded spike trains are fed into the input layer of the network.\n",
    "- The input layer neurons integrate incoming spikes over time according to the leaky-integrate-and-fire dynamics. Each neuron computes its membrane potential based on the incoming spikes and the leakage of charge over time.\n",
    "- Once the membrane potential of a neuron crosses a certain threshold, it generates an actional potential (or spike) and resets its membrane potential to a resting state.\n",
    "- The spikes generated by neurons in the input layer propagate to neurons in the hidden layer, where the process of integration and spike generation repeats.\n",
    "- This propagation then reaches the output layer.\n",
    "\n",
    "**Output Layer:**\n",
    "- Neurons in the output layer integrate incoming spikes from the preceding layer and produce output spikes according to their membrane potential dynamics.\n",
    "- The spike trains generated by neurons in the output layer are decoded to obtain continuous-valued output values. This encoding can be achieved using techniques such as population coding or temporal decoding.\n",
    "\n",
    "Diagram:\n",
    "`\n",
    "Continuous-valued Inputs -> Input Encoding -> Input Layer (LIF Neurons) -> Hidden Layers (LIF Neurons) -> Output Layer (LIF Neurons) -> Output Decoding -> Continuous-valued Output\n",
    "`\n",
    "\n",
    "Each layer of the network consists of LIF neurons that integrate incoming spikes and generate output spikes based on their membrane potential dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Elements of Constructing Feedforward Networks\n",
    "In this exercise, you will implement the two fundamental components of a feedforward spiking neural network: i) layers of neurons and ii) connections between those layers\n",
    "## 2a. \n",
    "As the first step towards creating an SNN, we will create a class that defines a layer of LIF neurons. The layer object creates a collection of LIF neurons and applies input current to it (also called psp_input for postsynaptic input) to produce the collective spiking output of the layer. \n",
    "\n",
    "Below is the class definition for a layer of LIFNeurons. Fill in the components to define the layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" Define Leaky Integrate-and-Fire Neuron Layer \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer.\n",
    "            vdecay (float): voltage decay of LIF neurons.\n",
    "            vth (float): voltage threshold of LIF neurons.\n",
    "        \n",
    "        This function is complete. You do not need to do anything here.\n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states.\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): Synaptic input current at a single timestep. The shape of this is same as the number of neurons in the layer. \n",
    "        Return:\n",
    "            self.spike: Output spikes from the layer. The shape of this should be the same as the number of neurons in the layer. \n",
    "        \n",
    "        Write the expressions for updating the voltage and generating the spikes for the layer given psp_input at one timestep. \n",
    "        \"\"\"\n",
    "        # Update the voltage.\n",
    "        self.volt = self.vdecay * self.volt + psp_input\n",
    "        \n",
    "        # Generate the spikes from the voltage.\n",
    "        self.spike = (self.volt >= self.vth).astype(int)\n",
    "        \n",
    "        # Reset the voltage if the neuron spikes.\n",
    "        self.volt[self.volt >= self.vth] = 0\n",
    "        \n",
    "        return self.spike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a layer of neurons using the class definition above, and pass through it random inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spikes: [1 0 1 0 0 1 0 1 1 0 1 1 1 0 0]\n",
      "Output spikes: [1 0 1 0 0 1 0 1 1 0 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Create a layer of neurons using the class definition above. You can pick any parameter values for the neurons. \n",
    "dimension = 15 # Layer contains 5 LIF neurons.\n",
    "vdecay = 0.75\n",
    "vth = 0.75\n",
    "layer = LIFNeurons(dimension, vdecay, vth)\n",
    "\n",
    "# Create random input spikes with any probability and print them.\n",
    "# Numpy random.choice function might be useful here. \n",
    "input_random_spikes = np.random.choice(2, dimension)\n",
    "print(\"Input spikes:\", input_random_spikes)\n",
    "\n",
    "# Propagate the random input spikes through the layer and print the output.\n",
    "output_random_spikes = layer(input_random_spikes)\n",
    "print(\"Output spikes:\", output_random_spikes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b.\n",
    "Now, we will create a class the defines the connection between a presynaptic layer and a postsynaptic layer. To create the connection, we need the activity of the presynaptic layer (also called presynaptic layer activation) and the weight matrix connecting the presynaptic and postsynaptic neurons. The output of the class should be the current for the postsynaptic layer. \n",
    "\n",
    "Below is the class definition for Connections. Fill in the components to create the connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pre_dimension (int): Number of neurons in the presynaptic layer.\n",
    "            post_dimension (int): Number of neurons in the postsynaptic layer.\n",
    "            weights (ndarray): Connection weights of shape post_dimension x pre_dimension.\n",
    "\n",
    "        This function is complete. You do not need to do anything here.\n",
    "\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): Spikes generated by the pre-synaptic neurons.\n",
    "        Return:\n",
    "            psp: Current for the post-synaptic neurons.\n",
    "        \n",
    "        Write the operation for computing psp.\n",
    "        \"\"\"\n",
    "        # Compute psp given spike_input and self.weights.\n",
    "        psp = np.dot(spike_input, np.transpose(self.weights))\n",
    "        return psp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, create a connection object and compute the postsynaptic current for random presynaptic activation inputs and random connection weights. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input spikes: [1 0 0 0 1 0 0 0 1 1 0 1 0 1 1]\n",
      "Weight matrix:\n",
      " [[0.26391067 0.55366929 0.3168378  0.13303788 0.90770484 0.6312124\n",
      "  0.29965567 0.22770978 0.83435587 0.64517841 0.13746944 0.83242934\n",
      "  0.38821789 0.37660779 0.09378695]\n",
      " [0.00250425 0.96725963 0.10562083 0.26514974 0.17059024 0.41420219\n",
      "  0.63261446 0.68923838 0.19734805 0.81045031 0.59879075 0.97231647\n",
      "  0.08918538 0.03986981 0.78913102]\n",
      " [0.58828671 0.21071273 0.72032937 0.58988139 0.1796041  0.72278894\n",
      "  0.25409031 0.24071129 0.3490788  0.98012182 0.39217624 0.84857443\n",
      "  0.38422023 0.81411904 0.63597258]\n",
      " [0.71034475 0.0302767  0.47419059 0.02722505 0.70278506 0.00731493\n",
      "  0.53194681 0.05236041 0.83843408 0.34066595 0.04489991 0.78290076\n",
      "  0.71728313 0.2438638  0.23712024]\n",
      " [0.86013299 0.05630427 0.89600343 0.96446268 0.3866694  0.88353065\n",
      "  0.08458162 0.76968445 0.86356813 0.13837095 0.25429938 0.68780056\n",
      "  0.83820123 0.87912276 0.03760556]]\n",
      "Postsynaptic curent: [3.95397386 2.98221016 4.39575748 3.85611463 3.85327036]\n",
      "Postsynaptic curent shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of the presynaptic layer in a variable.\n",
    "presynaptic_dimension = 15\n",
    "\n",
    "# Define the dimensions of the postsynaptic layer in a variable.\n",
    "postsynaptic_dimension = 5\n",
    "\n",
    "# Create random presynaptic inputs with any probability.\n",
    "# Numpy random choice function might be useful here.\n",
    "presynaptic_input_spikes = np.random.choice(2, presynaptic_dimension)\n",
    "print(\"Input spikes:\", presynaptic_input_spikes)\n",
    "\n",
    "# Create a random connection weight matrix.\n",
    "# Numpy random rand function might be useful here.\n",
    "weight_matrix = np.random.rand(postsynaptic_dimension, presynaptic_dimension)\n",
    "print(\"Weight matrix:\\n\", weight_matrix)\n",
    "\n",
    "# Initialize a connection object using the Connection class definition and pass the variables created above as arguments.\n",
    "connection = Connections(weight_matrix, presynaptic_dimension, postsynaptic_dimension)\n",
    "\n",
    "# Compute the current for the postsynaptic layer when the connection object is fed random presynaptic activation inputs.\n",
    "postsynaptic_current = connection(presynaptic_input_spikes)\n",
    "print(\"Postsynaptic curent:\", postsynaptic_current)\n",
    "\n",
    "# Print the shape of the current.\n",
    "print(\"Postsynaptic curent shape:\", postsynaptic_current.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Constructing Feedforward SNN\n",
    "Now that you have implemented the basic elements of an SNN- layer and connection, you are all set to implement a fully functioning SNN. The SNN that you will implement here consists of an input layer, a hidden layer, and an output layer. \n",
    "\n",
    "Below is the class definition of an SNN. Your task is to create the layers and connections that form the network using the class definitions in Question 2. Then complete the function to propagate a given input through the network and decode network output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with One Hidden Layer \"\"\"\n",
    "    def __init__(self, input_2_hidden_weight, hidden_2_output_weight, \n",
    "                 input_dimension=784, hidden_dimension=256, output_dimension=10,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer. dimension should be hidden_dimension x input_dimension. \n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer. dimension should be output dimension x hidden dimension. \n",
    "            input_dimension (int): number of neurons in the input layer\n",
    "            hidden_dimension (int): number of neurons in the hidden layer\n",
    "            output_dimension (int): number of neurons in the output layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "            snn_timestep (int): number of timesteps for simulating the network (also called inference timesteps)\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        \n",
    "        # Create the hidden layer.\n",
    "        self.hidden_layer = LIFNeurons(hidden_dimension, vdecay, vth)\n",
    "        \n",
    "        # Create the output layer.\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        \n",
    "        # Create the connection between input and hidden layer.\n",
    "        self.input_2_hidden_connection = Connections(input_2_hidden_weight, input_dimension, hidden_dimension)\n",
    "        \n",
    "        # Create the connection between hidden and output layer.\n",
    "        self.hidden_2_output_connection = Connections(hidden_2_output_weight, hidden_dimension, output_dimension)\n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            output: decoded output from the network\n",
    "        \"\"\"\n",
    "        # Initialize an array to store the decoded network output for all neurons in the output layer.\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "                                                    \n",
    "        #Loop through the simulation timesteps and process the input at each timestep tt\n",
    "        for tt in range(self.snn_timestep):\n",
    "            # Propagate the input through the input to hidden layer and compute current for hidden layer.\n",
    "            hidden_layer_current = self.input_2_hidden_connection(spike_encoding[tt])\n",
    "           \n",
    "            # Compute hidden layer spikes.\n",
    "            hidden_layer_spikes = self.hidden_layer(hidden_layer_current)\n",
    "            \n",
    "            # Propagate hidden layer inputs to output layer and compute current for output layer.\n",
    "            output_layer_current = self.hidden_2_output_connection(hidden_layer_spikes)\n",
    "            \n",
    "            # Compute output layer spikes.\n",
    "            output_layer_spikes = self.output_layer(output_layer_current)\n",
    "            \n",
    "            # Decode spike outputs by summing them up.\n",
    "            spike_output = spike_output + output_layer_spikes\n",
    "            \n",
    "        return spike_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, define the arguments to initialize the SNN. Then initialize the SNN and pass through it random inputs and compute network outputs. You can pick arbitrary values for class arguments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " [[1 0 1 0 0 1 0 1 0 1 1 0 1 1 0]\n",
      " [1 1 1 0 1 1 1 0 1 1 1 0 1 1 1]\n",
      " [0 1 1 0 0 0 0 1 0 1 1 0 1 0 1]\n",
      " [0 1 1 1 0 0 0 0 1 0 0 1 0 1 0]\n",
      " [0 0 0 1 0 1 0 1 1 0 1 1 1 0 1]\n",
      " [0 0 1 1 1 0 0 1 0 1 1 1 1 0 0]\n",
      " [1 0 1 0 1 0 1 1 0 0 1 1 1 1 0]\n",
      " [0 0 1 1 1 0 1 1 0 0 1 1 0 1 0]\n",
      " [1 0 0 0 1 0 1 0 1 0 0 1 0 0 1]\n",
      " [0 0 1 1 1 1 1 1 0 0 0 0 1 1 1]\n",
      " [1 1 1 1 1 0 0 1 0 0 0 0 1 0 1]\n",
      " [1 1 0 0 0 1 1 0 0 0 1 1 0 1 0]\n",
      " [0 0 0 0 1 1 0 1 0 0 1 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 1 1 1 0 0 1 0 0]\n",
      " [1 1 1 1 1 0 1 0 1 0 0 1 1 1 1]\n",
      " [1 0 0 1 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 1 0 0 1 0 1 0 1 1 0 0 0 1]\n",
      " [1 1 0 1 1 0 0 1 0 1 0 0 1 0 0]\n",
      " [0 1 1 0 0 1 1 0 0 0 0 1 0 0 1]\n",
      " [1 0 1 0 1 1 1 0 1 1 0 0 1 1 1]]\n",
      "Output:\n",
      " [20. 20. 20. 20. 20.]\n"
     ]
    }
   ],
   "source": [
    "# Define the dimensions of the input layer in a variable.\n",
    "input_dim = 15\n",
    "\n",
    "# Define the dimensions of the hidden layer in a variable.\n",
    "hidden_dim = 10\n",
    "\n",
    "# Define the dimensions of the output layer in a variable.\n",
    "output_dim = 5\n",
    "\n",
    "# Define vdecay in a variable.\n",
    "vdecay = 0.5\n",
    "\n",
    "# Define vth in a variable.\n",
    "vth = 0.5\n",
    "\n",
    "# Define snn_timesteps in a variable.\n",
    "snn_timestep = 20\n",
    "\n",
    "# Create random input to hidden layer weights.\n",
    "# Numpy random rand function might be useful here.\n",
    "input_2_hidden_weight = np.random.rand(hidden_dim, input_dim)\n",
    "\n",
    "# Create random hidden to output layer weights.\n",
    "# Numpy random rand function might be useful here.\n",
    "hidden_2_output_weight = np.random.rand(output_dim, hidden_dim)\n",
    "\n",
    "# Create random spike inputs to the network.\n",
    "# Numpy random choice function might be useful here\n",
    "spike_inputs = np.random.choice(2, (snn_timestep, input_dim))\n",
    "\n",
    "# Print the inputs.\n",
    "print('Input:\\n', spike_inputs)\n",
    "\n",
    "# Create an SNN object using the class definition and variables defined above.\n",
    "SNN_object = SNN(input_2_hidden_weight, hidden_2_output_weight,\n",
    "                 input_dim, hidden_dim, output_dim,\n",
    "                 vdecay, vth, snn_timestep)\n",
    "\n",
    "# Pass the random spike inputs through the SNN and print the output of the SNN.\n",
    "output = SNN_object.__call__(spike_inputs)\n",
    "print('Output:\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: SNN for Classification of Digits\n",
    "So far we have learnt how to construct SNNs for random inputs. In this exercise, you will use your implementation of SNNs to classify real-world data, taking the dataset of handwritten digits as an example. The dataset is provided as numpy arrays in the folder \"data\". Each sample in the MNIST dataset is a 28x28 image of a digit and a label (between 0 and 9) of that image. We will be dealing with batches, which means that we will read a fixed number of samples from the dataset (also called the batch size).\n",
    "\n",
    "## 4a. \n",
    "First, we need to write two helper functions- to read the data from the saved data files, and to convert an image into spikes. The function to read the data is already written for you. You need to complete the function for encoding the data into spikes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_numpy_mnist_data(save_root, num_sample):\n",
    "    \"\"\"\n",
    "    Read saved numpy MNIST data\n",
    "    Args:\n",
    "        save_root (str): path to the folder where the MNIST data is saved\n",
    "        num_sample (int): number of samples to read\n",
    "    Returns:\n",
    "        image_list: list of MNIST image\n",
    "        label_list: list of corresponding labels\n",
    "    \n",
    "    This function is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "    image_list = np.zeros((num_sample, 28, 28))\n",
    "    label_list = []\n",
    "    for ii in range(num_sample):\n",
    "        image_label = pickle.load(open(save_root + '/' + str(ii) + '.p', 'rb'))\n",
    "        image_list[ii] = image_label[0]\n",
    "        label_list.append(image_label[1])\n",
    "\n",
    "    return image_list, label_list\n",
    "\n",
    "def img_2_event_img(image, snn_timestep):\n",
    "    \"\"\"\n",
    "    Transform image to spikes, also called an event image\n",
    "    Args:\n",
    "        image (ndarray): image of shape batch_size x 28 x 28\n",
    "        snn_timestep (int): spike timestep\n",
    "    Returns:\n",
    "        event_image: event image- spike encoding of the image\n",
    "        \n",
    "    Complete the expression for converting the image to spikes (event image)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Reshape the image. Do not touch this code.\n",
    "    batch_size = image.shape[0]\n",
    "    image_size = image.shape[2]\n",
    "    image = image.reshape(batch_size, image_size, image_size, 1)\n",
    "    \n",
    "    # Generate a random image of the shape batch_size x image_size x image_size x snn_timestep.\n",
    "    # Numpy random rand function will be useful here.\n",
    "    random_image = np.random.rand(batch_size, image_size, image_size, snn_timestep)\n",
    "    \n",
    "    # Generate the event image.\n",
    "    temp = random_image.transpose(0, 3, 1, 2).reshape(batch_size, snn_timestep, -1)\n",
    "    event_image = (image.reshape(batch_size, -1)[:, np.newaxis, :] > temp).astype(int)\n",
    "    \n",
    "    return event_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To verify the correctness of your class implementation, load a sample digit from the saved file and convert it into an event image. Then print the shape of the event image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28)\n",
      "(1000, 20, 784)\n"
     ]
    }
   ],
   "source": [
    "# Load 1000 samples from the MNIST dataset using the read function defined above.\n",
    "image_list, label_list = read_numpy_mnist_data(\"data/mnist_test\", 1000)\n",
    "\n",
    "# Print the shape of the data.\n",
    "print(image_list.shape)\n",
    "\n",
    "# Convert the images to event images.\n",
    "event_image_list = img_2_event_img(image_list, snn_timestep)\n",
    "\n",
    "# Print the shape of the event image.\n",
    "print(event_image_list.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Next, we need another helper function to compute the classification accuracy of the network. The classification accuracy is defined as the percentage of the samples that the network classifies correctly. To compute the classification accuracy, you need to:\n",
    "\n",
    "- Propagate each input through the network and obtain the network output.\n",
    "- Based on the network output, the class of the image is the one for which the output neuron has maximum value. Let's call this predicted class. \n",
    "- Compare the predicted class against the true class. \n",
    "- Compute accuracy as the percentage of correct predictions. \n",
    "\n",
    "Below is the function for computing the test accuracy. The function takes in as arguments the SNN, directory in which the MNIST data is saved, and the number of samples to take from the MNIST dataset. Your task is to use the helper functions created above to load the data, convert into event images, and then compute network prediction and accuracies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_snn_with_mnist(network, data_save_dir, data_sample_num):\n",
    "    \"\"\"\n",
    "    Test SNN with MNIST test data\n",
    "    Args:\n",
    "        network (SNN): defined SNN network\n",
    "        data_save_dir (str): directory for the test data\n",
    "        data_sample_num (int): number of test data examples\n",
    "    \"\"\"\n",
    "    # Read image and labels using the read function.\n",
    "    test_image_list, test_label_list = read_numpy_mnist_data(data_save_dir, data_sample_num)\n",
    "    \n",
    "    # Convert the images to event images.\n",
    "    test_event_image_list = img_2_event_img(test_image_list, snn_timestep)\n",
    "    \n",
    "    # Initialize number of correct predictions to 0.\n",
    "    correct_prediction = 0\n",
    "    \n",
    "    # Loop through the test images.\n",
    "    for ii in range(data_sample_num):\n",
    "        # Compute network output for each image.\n",
    "        # You might have to reshape the image using Numpy reshape function so that its appropriate for the SNN.\n",
    "        network_output = network(test_event_image_list[ii])\n",
    "        \n",
    "        # Determine the class of the image from the network output.\n",
    "        # Numpy argmax function might be useful here.\n",
    "        classes = np.argmax(network_output)\n",
    "        \n",
    "        # Compare the predicted class against true class and update correct_prediction counter.\n",
    "        if classes == test_label_list[ii]:\n",
    "            correct_prediction += 1\n",
    "    \n",
    "    # Compute test accuracy.\n",
    "    test_accuracy = correct_prediction / data_sample_num\n",
    "    print(correct_prediction, '/', data_sample_num)\n",
    "    \n",
    "    return test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Tuning Membrane Properties for Correct Classification \n",
    "Great! We have everything that we need to measure the performance of the SNN for classification of MNIST digits. For this, we first need to create the SNN using the class definition we wrote in Q.3. Then we need to call the test function that we wrote in Q.4b. However, note that the SNN needs the connection weights between the layers as inputs. These weights are typically obtained as a result of \"training\" the network for a given task (such as MNIST classification). However, since training the network isn't a part of this assignment, we provide to you already trained weights. \n",
    "\n",
    "## 5a. \n",
    "Your task in this exercise is to initialize an SNN with vdecay=1.0 and vth=0.5. Test the SNN on MNIST dataset and obtain the classification accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241 / 1000\n",
      "0.241\n"
     ]
    }
   ],
   "source": [
    "# Load the weights. Do not touch this code.\n",
    "snn_param_dir = 'save_models/snn_bptt_mnist_train.p'\n",
    "snn_param_dict = pickle.load(open(snn_param_dir, 'rb'))\n",
    "input_2_hidden_weight = snn_param_dict['weight1']\n",
    "hidden_2_output_weight = snn_param_dict['weight2']\n",
    "\n",
    "# Define a variable for vdecay.\n",
    "vdecay = 1.0\n",
    "\n",
    "# Define a variable for vth.\n",
    "vth = 0.5\n",
    "\n",
    "# Create the SNN using the class definition in Q3 and the variables defined above.\n",
    "input_dimension = 784\n",
    "hidden_dimension = 256\n",
    "output_dimension = 10\n",
    "snn_timestep = 20\n",
    "\n",
    "SNN_object2 = SNN(input_2_hidden_weight, hidden_2_output_weight,\n",
    "              input_dimension, hidden_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Compute test accuracy for the SNN on 1000 examples from MNIST dataset and print it.\n",
    "test_accuracy = test_snn_with_mnist(SNN_object2, \"data/mnist_test\", 1000)\n",
    "print(test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could be a possible reason for the poor accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5a. \n",
    "Referring back to the method that defines an SNN (__init__) in the class SNN, note that the parameter vdecay represents the decay factor for the membrane potential (voltage) of the neurons in the network. It is set to a value of 0.5 which suggests that the voltage decays by half each time step. However, when vdecay = 1.0, the membrane potential of the neurons in the network will not decay over time. This means that the voltage at each timestep will remain the same, thus affecting the behavior of the network during simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5b. \n",
    "Can you tune the membrane properties (vdecay and vth) to obtain higher classification accuracies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975 / 1000\n",
      "0.975\n"
     ]
    }
   ],
   "source": [
    "# Write your implementation of Question 5b here.\n",
    "vdecay = 0.4\n",
    "vth = 0.5\n",
    "\n",
    "input_dimension = 784\n",
    "hidden_dimension = 256\n",
    "output_dimension = 10\n",
    "snn_timestep = 20\n",
    "\n",
    "SNN_object3 = SNN(input_2_hidden_weight, hidden_2_output_weight,\n",
    "              input_dimension, hidden_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "test_accuracy2 = test_snn_with_mnist(SNN_object3, \"data/mnist_test\", 1000)\n",
    "print(test_accuracy2)\n",
    "\n",
    "# Student Note: I kept getting dimension errors, and I believe I fixed it.\n",
    "# But, just in case, if you want to try other vedecay values here, please do [Restart Kernal and Run All Cells] :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5c.\n",
    "Based on your response to Questions 5a and 5b, can you explain how membrane properties affect network activity for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 5c.\n",
    "Double click to enter your response to Question 5c here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membrane properties in SNNs, can have significant effects on network activity, which in turn can impact the network's ability to perform classification tasks. Specifically, decay dactor (vdecay) controls how quickly the membrane potential of a neuron decays over time. A higher decay factor (>= 1) means that the membrane potential decays more rapidly, influencing the neuron's responsiveness to input spikes. A lower decay factor (< 1, positive) means that the membrane potential decays more slowly, allowing the neuron to integrate input spikes over a longer period, affecting its sensitivity to temporal patterns in the input. Essentially, membrane properties determine spiking behavior, therefore the proper values are necessary for a valid SNN to perform classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Goals\n",
    "In Assignment 2, we learnt how to construct networks of spiking neurons and propagate information through a network of fixed weights. In this assignment, you will learn how to train network weights for a given task using brain-inspired learning rules.\n",
    "\n",
    "Let's import all the libraries required for this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1: Training a Network\n",
    "\n",
    "## 1a. \n",
    "What is the purpose of a learning algorithm? In other words, what does a learning algorithm dictate, and what is the objective of it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1a. \n",
    "Learning algorithms consist of a set of procedures designed to enable a computer program to learn from data and improve its performance over time. These algorithms are part of a subfield of artificial intelligence. It dictates how well a computer program can learn using a training model. The objective of learning algorithms is to observe that the rate of learning decreases with time during training, and to enable the algorithm to make formidable predictions during testing. When it comes to learning, two majors obstacles is choosing the correct loss function to measure reliance of the data and preventing overfitting.\n",
    "\n",
    "There are also various types of learning algorithms, including supervised and unsupervising learning. In my Introduction to AI course, our final project dealt with supervised learning! We had a 20 x 20 grid layout of a pixel image with four colored wires, two across and two down. These colored wires were laid randomly, and our goal was to cut the third wire laid in order to diffuse an impending bomb! We used a linear regression model for the first task, and multinomial regression for the second task. In our write-up, we had to clearly outline key components of a learning algorithm: input/output space, model space, loss function, type of regularization, training algorithm (or gradient descent), and testing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. \n",
    "Categorize and explain the various learning algorithms w.r.t. biological plausibility. Can you explain the tradeoffs involved with the different learning rules? *Hint: Think computational advantages and disadvantages of biological plausibility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 1b. \n",
    "As mentioned, two major types of learning algorithms are supervising learning versus unsupervised learning. Both can be discussed with respect to biological plausibility in the following manner.\n",
    "\n",
    "In supervised learning, the learning algorithm learns from direct data, where each input-output pair is given during training. After training, the algorithm is given new data where it learns to map inputs to outputs by classifying a label during testing. In terms of biological plausability, such a strict form of learning may not prove to be efficient because it tends to lead to overfitting to the training data, leading to complications when it comes to adapting to new data. While supervised learning will work for less complex mechanisms, it may not be the most suitable to datasets that are susceptible to change.\n",
    "\n",
    "In unsupervised learning, the learning algorithm learns patterns from unlabeled data. The algorithm tries to find hidden or intrinsic structures within the data, without human intervention, via clustering or dimensionality reduction. However, this type of learning leads to less accuracy, as opposed to supervised learning. In fact, a downside to unsupervised learning is computational complexity. In terms of biological plausability, this 'free'-form of learning may be more efficient because it allows for passive changes to be taken into account when conducting statistical correlations. Therefore, even if there are outliers in the data, they are still taken into account in the process of learning.\n",
    "\n",
    "Next, we talk about two specific learning algorithms: Hebbian learning and STDP.\n",
    "\n",
    "In Hebbian learning, it can best be summarized by \"neurons that fire together wire together\". In terms of biological plausability, this means that the connection between neurons are stronger when activated together, or weaker otherwise.\n",
    "\n",
    "In STDP, it is a type of Hebbian learning that takes into account temporal encoding. In terms of biological plasuability, the strength of a connection between neurons is modified based on the relative timing of spikes.\n",
    "\n",
    "More on Hebbian learning and STDP will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Hebbian Learning\n",
    "\n",
    "## 2a.\n",
    "\n",
    "In this exercise, you will implement the hebbian learning rule to solve AND Gate. First, we need to create a helper function to generate the training data. The function should return lists of tuples where each tuple comprises of numpy arrays of rate-coded inputs and the corresponding rate-coded output. \n",
    "\n",
    "Below is the function to generate the training data. Fill the components to return the training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genANDTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for AND.\n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of AND gate to the list train_data\n",
    "    \"\"\"\n",
    "    # Initialize an empty list for train data.\n",
    "    train_data = []\n",
    "    \n",
    "    # Encode 0. Numpy random choice function might be useful here.\n",
    "    zero = np.random.choice([1, 0], snn_timestep)\n",
    "    print('Zero:\\n', zero)\n",
    "    \n",
    "    # Encode 1. Numpy random choice function might be useful here.\n",
    "    one = np.random.choice([1, 0], snn_timestep)\n",
    "    print('One:\\n', one)\n",
    "    \n",
    "    # Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here.\n",
    "    sub_case1 = np.array([zero, zero])\n",
    "    sub_case2 = np.array([zero, one])\n",
    "    sub_case3 = np.array([one, zero])\n",
    "    sub_case4 = np.array([one, one])\n",
    "    \n",
    "    case1 = np.array([sub_case1, zero])\n",
    "    case2 = np.array([sub_case2, zero])\n",
    "    case3 = np.array([sub_case3, zero])\n",
    "    case4 = np.array([sub_case4, one])\n",
    "    \n",
    "    train_data = np.stack((case1, case2, case3, case4))\n",
    "    print('Training Data:\\n', train_data)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. \n",
    "We will use the implementation of the network from assignment 2 to create an SNN comprising of one input layer and one output layer. Can you explain algorithmically, how you can use this simple architecture to learn AND gate. Your algorithm should comprise of encoding, forward propagation, network training, and decoding steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 2b.\n",
    "First, we define that Hebbian learning is a type of unsupervised learning that is based on the idea that connections between neurons strengthen when those neurons fire together. Therefore, during learning, the connections of neurons that fire together strengthen, while the connections of neurons that do not fire together weaken.\n",
    "\n",
    "Second, we define an AND gate is an implementation of two input neurons and one output neuron. Each input neuron represents one of the binary inputs to the AND gate (0 or 1), and the output neuron represents the output of the gate (0 or 1). If throwback to Discrete Structures, we know that the expression (A & B) is only true if both A and B are true.\n",
    "\n",
    "We can now combine the two concepts together. When both input neurons are active, the output neuron should also be activated to produce a true output. Strengthening the synaptic connections between the input neurons and the output neuron when both inputs are active ensures that the output neuron is more likely to fire in response to this input pattern in the future. When both input neurons are not active or only _one_ input neuron is active, the output neuron should also not be as activated, if at all active.\n",
    "\n",
    "Using this fundamental idea, we can now begin to explain algorithmically how an SNN comprising of one input layer and one output layer can learn a simple logical gate.\n",
    "\n",
    "**Encoding:**\n",
    "First, we encode the input layer into spike trains. Each neuron in this layer is resembled by one input bit of the AND gate (0,0), (0,1), (1,0), or (1,1).\n",
    "\n",
    "**Forward Propagation:**\n",
    "Next, we propagate spikes from the input layer to the output layer. Each connection between an input neuron and an output neuron has a weight associated with it. The output neuron integrates the weighted spikes from the input neurons over time.\n",
    "\n",
    "**Training Data:**\n",
    "   - Initialize the connection weights connecting the input neurons to the output neurons with random values.\n",
    "   - Iterate over the training data and calculate the mean firing rate.\n",
    "   - Adjust the connection weights based on mean firing rate. This should strengthen the connections that contribute to correct outputs and weaken those that contribute to incorrect outputs.\n",
    "   - Run the model for specified number of epochs.\n",
    "\n",
    "**Decoding:**\n",
    "Lastly, we decode the output spikes to determine the predicted output of the AND gate. We can do so by using a threshold value: Over the threshold would be classified as 1, or 0 otherwise.\n",
    "\n",
    "**Testing Data:**\n",
    "Present new input data to the trained SNN and compare the decoded outputs with the expected outputs. We can measure the accuracy of the SNN in correctly predicting the output of the AND gate for different input patterns.\n",
    "\n",
    "For an AND gate, this means testing it with input patterns where both inputs are 0, one input is 0 while the other is 1, and both inputs are 1.\n",
    "\n",
    "If the SNN-based AND gate has learned correctly through Hebbian learning, it should produce the correct output pattern (0 or 1) for each corresponding input pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SNN has already been implemented for you. You do not need to do anything here. Just understand the implementation so that you can use it in the later parts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LIFNeurons:\n",
    "    \"\"\" \n",
    "        Define Leaky Integrate-and-Fire Neuron Layer \n",
    "        This class is complete. You do not need to do anything here.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimension, vdecay, vth):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimension (int): Number of LIF neurons in the layer\n",
    "            vdecay (float): voltage decay of LIF neurons\n",
    "            vth (float): voltage threshold of LIF neurons\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dimension = dimension\n",
    "        self.vdecay = vdecay\n",
    "        self.vth = vth\n",
    "\n",
    "        # Initialize LIF neuron states.\n",
    "        self.volt = np.zeros(self.dimension)\n",
    "        self.spike = np.zeros(self.dimension)\n",
    "    \n",
    "    def __call__(self, psp_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            psp_input (ndarray): synaptic inputs \n",
    "        Return:\n",
    "            self.spike: output spikes from the layer\n",
    "                \"\"\"\n",
    "        self.volt = self.vdecay * self.volt * (1. - self.spike) + psp_input\n",
    "        self.spike = (self.volt > self.vth).astype(float)\n",
    "        return self.spike\n",
    "\n",
    "class Connections:\n",
    "    \"\"\" Define connections between spiking neuron layers \"\"\"\n",
    "\n",
    "    def __init__(self, weights, pre_dimension, post_dimension):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            weights (ndarray): connection weights\n",
    "            pre_dimension (int): dimension for pre-synaptic neurons\n",
    "            post_dimension (int): dimension for post-synaptic neurons\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.pre_dimension = pre_dimension\n",
    "        self.post_dimension = post_dimension\n",
    "    \n",
    "    def __call__(self, spike_input):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_input (ndarray): spikes generated by the pre-synaptic neurons\n",
    "        Return:\n",
    "            psp: postsynaptic layer activations\n",
    "        \"\"\"\n",
    "        psp = np.matmul(self.weights, spike_input)\n",
    "        return psp\n",
    "    \n",
    "    \n",
    "class SNN:\n",
    "    \"\"\" Define a Spiking Neural Network with No Hidden Layer \"\"\"\n",
    "\n",
    "    def __init__(self, input_2_output_weight, \n",
    "                 input_dimension=2, output_dimension=2,\n",
    "                 vdecay=0.5, vth=0.5, snn_timestep=20):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_2_hidden_weight (ndarray): weights for connection between input and hidden layer\n",
    "            hidden_2_output_weight (ndarray): weights for connection between hidden and output layer\n",
    "            input_dimension (int): input dimension\n",
    "            hidden_dimension (int): hidden_dimension\n",
    "            output_dimension (int): output_dimension\n",
    "            vdecay (float): voltage decay of LIF neuron\n",
    "            vth (float): voltage threshold of LIF neuron\n",
    "            snn_timestep (int): number of timesteps for inference\n",
    "        \"\"\"\n",
    "        self.snn_timestep = snn_timestep\n",
    "        self.output_layer = LIFNeurons(output_dimension, vdecay, vth)\n",
    "        self.input_2_output_connection = Connections(input_2_output_weight, input_dimension, output_dimension)\n",
    "    \n",
    "    def __call__(self, spike_encoding):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            spike_encoding (ndarray): spike encoding of input\n",
    "        Return:\n",
    "            spike outputs of the network\n",
    "        \"\"\"\n",
    "        spike_output = np.zeros(self.output_layer.dimension)\n",
    "        for tt in range(self.snn_timestep):\n",
    "            input_2_output_psp = self.input_2_output_connection(spike_encoding[:, tt])\n",
    "            output_spikes = self.output_layer(input_2_output_psp)\n",
    "            spike_output += output_spikes\n",
    "        return spike_output/self.snn_timestep      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. \n",
    "Next, you need to write a function for network training using hebbian learning rule. The function is defined below. You need to fill in the components so that the network weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hebbian(network, train_data, lr=1e-5, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule.\n",
    "        Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over the epochs.\n",
    "    for ee in range(epochs):\n",
    "        # Iterate over all samples in train_data.\n",
    "        for data in train_data:\n",
    "            # Compute the firing rate for the input.\n",
    "            input_firing_rate = np.mean(data[0], 1)\n",
    "            \n",
    "            # Compute the firing rate for the output.\n",
    "            output_firing_rate = np.mean(data[1])\n",
    "            \n",
    "            # Compute the correlation using the firing rates calculated above.\n",
    "            correlation = input_firing_rate * output_firing_rate \n",
    "            \n",
    "            # Compute the weight increment.\n",
    "            weight_increment = lr * correlation\n",
    "            \n",
    "            # Increment the weight.\n",
    "            network.input_2_output_connection.weights += weight_increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2d. \n",
    "In this exercise, you will use your implementations above to train an SNN to learn AND gate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Network Weights:\n",
      " [[0.33186292 0.45042939]]\n",
      "\n",
      "***TRAINING***\n",
      "Zero:\n",
      " [0 1 1 1 0 1 1 1 1 0]\n",
      "One:\n",
      " [1 1 1 0 1 0 0 0 0 1]\n",
      "Training Data:\n",
      " [[array([[0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
      "       [0, 1, 1, 1, 0, 1, 1, 1, 1, 0]])\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0])]\n",
      " [array([[0, 1, 1, 1, 0, 1, 1, 1, 1, 0],\n",
      "       [1, 1, 1, 0, 1, 0, 0, 0, 0, 1]])\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0])]\n",
      " [array([[1, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
      "       [0, 1, 1, 1, 0, 1, 1, 1, 1, 0]])\n",
      "  array([0, 1, 1, 1, 0, 1, 1, 1, 1, 0])]\n",
      " [array([[1, 1, 1, 0, 1, 0, 0, 0, 0, 1],\n",
      "       [1, 1, 1, 0, 1, 0, 0, 0, 0, 1]])\n",
      "  array([1, 1, 1, 0, 1, 0, 0, 0, 0, 1])]]\n",
      "\n",
      "***TESTING***\n",
      "Zero:\n",
      " [0 1 1 0 0 1 1 1 1 0]\n",
      "One:\n",
      " [1 0 0 0 0 1 1 0 0 1]\n",
      "Case 1: [0.6]\n",
      "Case 2: [0.8]\n",
      "Case 3: [0.8]\n",
      "Case 4: [0.4]\n",
      "Testing Data:\n",
      " [1, 1, 1, 0]\n",
      "\n",
      "Final Network Weights:\n",
      " [[0.64786292 0.76642939]]\n"
     ]
    }
   ],
   "source": [
    "# Define a variable for input dimension.\n",
    "input_dimension = 2\n",
    "\n",
    "# Define a variable for output dimension.\n",
    "output_dimension = 1\n",
    "\n",
    "# Define a variable for voltage decay.\n",
    "vdecay = 0.5\n",
    "\n",
    "# Define a variable for voltage threshold.\n",
    "vth = 0.5\n",
    "\n",
    "# Define a variable for snn timesteps.\n",
    "snn_timestep = 10\n",
    "\n",
    "# Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
    "\n",
    "# Print the initial network weights.\n",
    "print('\\nInitial Network Weights:\\n', input_2_output_weight)\n",
    "\n",
    "# Initialize a SNN using the arguments defined above.\n",
    "snn_1 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Get the training data for AND gate using the function defined in 2a.\n",
    "print('\\n***TRAINING***')\n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "# Train the network using the function defined in 2c with the appropriate arguments.\n",
    "hebbian(snn_1, train_data, lr=2e-2, epochs=10)\n",
    "\n",
    "# Test the trained network and print the network output for all 4 cases.\n",
    "print('\\n***TESTING***')\n",
    "test_data = []\n",
    "\n",
    "zero = np.random.choice([1, 0], snn_timestep)\n",
    "print('Zero:\\n', zero)\n",
    "\n",
    "one = np.random.choice([1, 0], snn_timestep)\n",
    "print('One:\\n', one)\n",
    "\n",
    "case1 = np.array([zero, zero])\n",
    "case2 = np.array([zero, one])\n",
    "case3 = np.array([one, zero])\n",
    "case4 = np.array([one, one])\n",
    "\n",
    "# Case 1:\n",
    "output1 = snn_1(case1)\n",
    "print(\"Case 1:\", output1)\n",
    "if output1 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 2:\n",
    "output2 = snn_1(case2)\n",
    "print(\"Case 2:\", output2)\n",
    "if output2 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 3:\n",
    "output3 = snn_1(case3)\n",
    "print(\"Case 3:\", output3)\n",
    "if output3 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 4:\n",
    "output4 = snn_1(case4)\n",
    "print(\"Case 4:\", output4)\n",
    "if output4 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "print('Testing Data:\\n', test_data)\n",
    "\n",
    "# Print the final network weights.\n",
    "print(\"\\nFinal Network Weights:\\n\", snn_1.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3: Limitations of Hebbian Learning rule\n",
    "\n",
    "## 3a. \n",
    "Can you learn the AND gate using 2 neurons in the output layer instead of one? If yes, describe what changes you might need to make to your algorithm in 2b. If not, explain why not, and what consequences it might entail for the use of hebbian learning for complex real-world tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3a. \n",
    "We can use two neurons in the output layer! However, since this is Hebbian learning, we would need to adjust the weight update rule to accommodate multiple output neurons by using negative weights. Negative weights enable the SNN to learn that certain input combinations should result in a higher activation ('strengthen') for specific output neurons. So, in the decoding phase: Over the threshold would be classified as 0.5 (rather than 1), or -0.5 (instead of 0) otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3b. \n",
    "Train the network using hebbian learning for AND gate with the same arguments as defined in 2d. but now multiply the number of epochs by 20. Can your network still learn AND gate correctly? Inspect the initial and final network weights, and compare them against the network weights in 2d. Based on this, explain your observations for the network behavior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***TRAINING***\n",
      "Zero:\n",
      " [0 0 0 1 0 1 0 1 0 1]\n",
      "One:\n",
      " [0 1 0 1 0 0 1 1 0 1]\n",
      "Training Data:\n",
      " [[array([[0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1]])\n",
      "  array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])]\n",
      " [array([[0, 0, 0, 1, 0, 1, 0, 1, 0, 1],\n",
      "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])\n",
      "  array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])]\n",
      " [array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
      "       [0, 0, 0, 1, 0, 1, 0, 1, 0, 1]])\n",
      "  array([0, 0, 0, 1, 0, 1, 0, 1, 0, 1])]\n",
      " [array([[0, 1, 0, 1, 0, 0, 1, 1, 0, 1],\n",
      "       [0, 1, 0, 1, 0, 0, 1, 1, 0, 1]])\n",
      "  array([0, 1, 0, 1, 0, 0, 1, 1, 0, 1])]]\n",
      "\n",
      "***TESTING***\n",
      "Zero:\n",
      " [0 1 0 0 0 0 1 1 0 1]\n",
      "One:\n",
      " [0 1 1 0 1 0 0 0 0 1]\n",
      "Case 1: [0.4]\n",
      "Case 2: [0.6]\n",
      "Case 3: [0.6]\n",
      "Case 4: [0.4]\n",
      "Testing Data:\n",
      " [0, 1, 1, 0]\n",
      "\n",
      "Final Network Weights:\n",
      " [[3.72786292 3.84642939]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation for 3b (Same as 2d, but with change of one argument).\n",
    "\n",
    "# ...\n",
    "# Initialize a SNN using the arguments defined above.\n",
    "snn_2 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Get the training data for AND gate using the function defined in 2a.\n",
    "print('\\n***TRAINING***')\n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "# Train the network using the function defined in 2c with the appropriate arguments.\n",
    "hebbian(snn_2, train_data, lr=2e-2, epochs=200) # 10 * 20 = 200\n",
    "\n",
    "# Test the trained network and print the network output for all 4 cases.\n",
    "print('\\n***TESTING***')\n",
    "test_data = []\n",
    "\n",
    "zero = np.random.choice([1, 0], snn_timestep)\n",
    "print('Zero:\\n', zero)\n",
    "\n",
    "one = np.random.choice([1, 0], snn_timestep)\n",
    "print('One:\\n', one)\n",
    "\n",
    "case1 = np.array([zero, zero])\n",
    "case2 = np.array([zero, one])\n",
    "case3 = np.array([one, zero])\n",
    "case4 = np.array([one, one])\n",
    "\n",
    "# Case 1:\n",
    "output1 = snn_2(case1)\n",
    "print(\"Case 1:\", output1)\n",
    "if output1 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 2:\n",
    "output2 = snn_2(case2)\n",
    "print(\"Case 2:\", output2)\n",
    "if output2 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 3:\n",
    "output3 = snn_2(case3)\n",
    "print(\"Case 3:\", output3)\n",
    "if output3 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 4:\n",
    "output4 = snn_2(case4)\n",
    "print(\"Case 4:\", output4)\n",
    "if output4 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "print('Testing Data:\\n', test_data)\n",
    "\n",
    "# Print the final network weights.\n",
    "print(\"\\nFinal Network Weights:\\n\", snn_2.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3b. \n",
    "In `2d`, when `epochs=10`, the final network weights were generally in the range between 0 and 1 (more or less inclusively). In `3b`, when `epochs=200`, the final network weights were generally much greater than 1. This shows a drastic difference in weight calculation when comparing the respective initial weights to the final weights.\n",
    "\n",
    "Such a change in weightsÂ is likely an indicator of stronger connections between the input and output neurons. We refer back to `2b` where we state that our main goal with the AND gate is to strengthen the connection weights between the input neurons and output neurons. With the increase in weight, we can see that increasing epochs enabled that goal. So, our network can, indeed, still learn and make better predictions! However, this might be resulting a positive feedback loop. With no bounds in place, it seems that the connection weights are able to grow without fault over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3c. \n",
    "Based on your observations and response in 3b., can you explain another limitation of hebbian learning rule w.r.t. weight growth? Can you also suggest a possible remedy for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 3c. \n",
    "Another limitation of hebbian learning with respect to weight growth is oversaturation. As we observed in `3b`, the connection weights of the network grew without bounds. This could lead to neurons saturating as they reach their maximum activation levels. This saturation can prevent further learning and limit the network's capacity to adapt to new information.\n",
    "\n",
    "To remedy this, (after possibly sneaking a peek at 3d...), one approach is Oja's rule. Oja's rule is useful for principal component analysis (PCA) and for extracting the principal components of the input data. It operates on a single neuron and updates its weight vector in the direction of the input vector that has the maximal variance with the current weight vector. We do this because maximum variances leads to maximum information. By focusing on the most informative dimensions of the input data, Oja's rule can lead to more efficient learning and adaptation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3d. \n",
    "To resolve the issues with hebbian learning, one possibility is Oja's rule. In this exercise, you will implement and train an SNN using Oja's learning rule. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oja(network, train_data, lr=1e-5, epochs=10):\n",
    "    \"\"\" \n",
    "    Function to train a network using Hebbian learning rule.\n",
    "        Args:\n",
    "            network (SNN): SNN network object\n",
    "            train_data (list): training data \n",
    "            lr (float): learning rate\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples. \n",
    "        \n",
    "        Write the operations required to compute the weight increment according to the hebbian learning rule. Then increment the network weights. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Iterate over the epochs.\n",
    "    for ee in range(epochs):\n",
    "        # Iterate over all samples in train_data.\n",
    "        for data in train_data:\n",
    "            # Compute the firing rate for the input.\n",
    "            input_firing_rate = np.mean(data[0], 1)\n",
    "            \n",
    "            # Compute the firing rate for the output.\n",
    "            output_firing_rate = np.mean(data[1])\n",
    "            \n",
    "            # Compute the weight increment.\n",
    "            weight_increment = lr * np.outer(output_firing_rate, input_firing_rate)\n",
    "            \n",
    "            # Increment the weight.\n",
    "            network.input_2_output_connection.weights += weight_increment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, test your implementation below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Network Weights:\n",
      " [[0.78223878 0.0657187 ]]\n",
      "\n",
      "***TRAINING***\n",
      "Zero:\n",
      " [0 0 1 1 1 1 1 1 1 0]\n",
      "One:\n",
      " [0 0 0 0 0 0 1 1 0 1]\n",
      "Training Data:\n",
      " [[array([[0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "  array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])]\n",
      " [array([[0, 0, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1]])\n",
      "  array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])]\n",
      " [array([[0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
      "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "  array([0, 0, 1, 1, 1, 1, 1, 1, 1, 0])]\n",
      " [array([[0, 0, 0, 0, 0, 0, 1, 1, 0, 1],\n",
      "       [0, 0, 0, 0, 0, 0, 1, 1, 0, 1]])\n",
      "  array([0, 0, 0, 0, 0, 0, 1, 1, 0, 1])]]\n",
      "\n",
      "***TESTING***\n",
      "Zero:\n",
      " [1 0 0 0 1 0 0 0 1 0]\n",
      "One:\n",
      " [1 1 0 1 1 0 1 1 0 0]\n",
      "Case 1: [0.3]\n",
      "Case 2: [0.3]\n",
      "Case 3: [0.6]\n",
      "Case 4: [0.6]\n",
      "Testing Data:\n",
      " [0, 0, 1, 1]\n",
      "\n",
      "Final Network Weights:\n",
      " [[1.03823878 0.3217187 ]]\n"
     ]
    }
   ],
   "source": [
    "# Define a variable for input dimension.\n",
    "input_dimension = 2\n",
    "\n",
    "# Define a variable for output dimension.\n",
    "output_dimension = 1\n",
    "\n",
    "# Define a variable for voltage decay.\n",
    "vdecay = 0.5\n",
    "\n",
    "# Define a variable for voltage threshold.\n",
    "vth = 0.5\n",
    "\n",
    "# Define a variable for snn timesteps.\n",
    "snn_timestep = 10\n",
    "\n",
    "# Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
    "\n",
    "# Print the initial network weights.\n",
    "print('\\nInitial Network Weights:\\n', input_2_output_weight)\n",
    "\n",
    "# Initialize a SNN using the arguments defined above.\n",
    "snn_3 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Get the training data for AND gate using the function defined in 2a.\n",
    "print('\\n***TRAINING***')\n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "# Train the network using the function defined in 3d with the appropriate arguments.\n",
    "oja(snn_3, train_data, lr=2e-2, epochs=10)\n",
    "\n",
    "# Test the trained network and print the network output for all 4 cases.\n",
    "print('\\n***TESTING***')\n",
    "test_data = []\n",
    "\n",
    "zero = np.random.choice([1, 0], snn_timestep)\n",
    "print('Zero:\\n', zero)\n",
    "\n",
    "one = np.random.choice([1, 0], snn_timestep)\n",
    "print('One:\\n', one)\n",
    "\n",
    "case1 = np.array([zero, zero])\n",
    "case2 = np.array([zero, one])\n",
    "case3 = np.array([one, zero])\n",
    "case4 = np.array([one, one])\n",
    "\n",
    "# Case 1:\n",
    "output1 = snn_3(case1)\n",
    "print(\"Case 1:\", output1)\n",
    "if output1 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 2:\n",
    "output2 = snn_3(case2)\n",
    "print(\"Case 2:\", output2)\n",
    "if output2 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 3:\n",
    "output3 = snn_3(case3)\n",
    "print(\"Case 3:\", output3)\n",
    "if output3 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 4:\n",
    "output4 = snn_3(case4)\n",
    "print(\"Case 4:\", output4)\n",
    "if output4 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "print('Testing Data:\\n', test_data)\n",
    "\n",
    "# Print the final network weights.\n",
    "print(\"\\nFinal Network Weights:\\n\", snn_3.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Spike-time dependent plasticity (STDP)\n",
    "\n",
    "## 4a. \n",
    "What is the limitation with hebbian learning that STDP aims to resolve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4a. \n",
    "Hebbian learning simply strengthens connection weights between neurons that fire together, without distinguishing between the relative timing of their activity. As a result, it cannot capture the causal relationship between the firing of pre- and postsynaptic neurons. Therefore, the limitation of hebbian learning that STDP aims to resolve is the precise timing of pre- and postsynaptic spikes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. \n",
    "Describe the algorithm to train a network using STDP learning rule. You do not need to describe encoding here. Your algorithm should be such that its naturally translatable to a program. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer 4b. \n",
    "According to the STDP model referenced in the article below, there is a weight change if there is a pre-synaptic spike in the temporal vicinity of a post-synaptic spike. The change is positive if the pre-synaptic spike occurs immediately _before_ the post-synaptic spike. Otherwise, the change is negative. Unlike hebbian learning, each connection weight in our network introduces a time delay, representing the difference between post-synaptic firing and pre-synaptic potential rise (where time in in discrete units). Membrane potential increases by the weight value of each incoming spike, but a constant is subtracted periodically to account for time delay. When the potential crosses the threshold, a spike is produced, and the potential resets to the resting level. This is the basic algorithm for STDP.\n",
    "\n",
    "Note that the same ideas from hebbian learning still apply, such as encoding, propagation, training, decoding, testing...etc. But, connection weights are updated much differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4c. \n",
    "In this exercise, you will implement the STDP learning algorithm to train a network. STDP has many different flavors. For this exercise, we will use the learning rule defined in: https://dl.acm.org/doi/pdf/10.1609/aaai.v33i01.330110021. Pay special attention to Equations 2 and 3. \n",
    "\n",
    "Below is the class definition for STDP learning algorithm. Your task is to fill in the components so that the weights are updated in the right manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STDP():\n",
    "    \"\"\"Train a network using STDP learning rule\"\"\"\n",
    "    def __init__(self, network, A_plus, A_minus, tau_plus, tau_minus, lr, snn_timesteps=20, epochs=30, w_min=0, w_max=1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            network (SNN): network which needs to be trained\n",
    "            A_plus (float): STDP hyperparameter\n",
    "            A_minus (float): STDP hyperparameter\n",
    "            tau_plus (float): STDP hyperparameter\n",
    "            tau_minus (float): STDP hyperparameter\n",
    "            lr (float): learning rate\n",
    "            snn_timesteps (int): SNN simulation timesteps\n",
    "            epochs (int): number of epochs to train with. Each epoch is defined as one pass over all training samples.  \n",
    "            w_min (float): lower bound for the weights\n",
    "            w_max (float): upper bound for the weights\n",
    "        \"\"\"\n",
    "        self.network = network\n",
    "        self.A_plus = A_plus\n",
    "        self.A_minus = A_minus\n",
    "        self.tau_plus = tau_plus\n",
    "        self.tau_minus = tau_minus\n",
    "        self.snn_timesteps = snn_timesteps\n",
    "        self.lr = lr\n",
    "        self.time = np.arange(0, self.snn_timesteps, 1)\n",
    "        self.sliding_window = np.arange(-4, 4, 1) # Defines a sliding window for STDP operation. \n",
    "        self.epochs = epochs\n",
    "        self.w_min = w_min\n",
    "        self.w_max = w_max\n",
    "    \n",
    "    def update_weights(self, t, i):\n",
    "        \"\"\"\n",
    "        Function to update the network weights using STDP learning rule.\n",
    "        Args:\n",
    "            t (int): time difference between postsynaptic spike and a presynaptic spike in a sliding window\n",
    "            i(int): index of the presynaptic neuron\n",
    "        \n",
    "        Fill the details of STDP implementation.\n",
    "        \"\"\"\n",
    "        # Compute delta_w for positive time difference.\n",
    "        if t > 0:\n",
    "            delta_w = self.A_plus * np.exp(-t / self.tau_plus)\n",
    "        \n",
    "        # Compute delta_w for negative time difference.\n",
    "        else:\n",
    "            delta_w = -self.A_minus * np.exp(-t / self.tau_minus)\n",
    "        \n",
    "        # Update the network weights if weight increment is negative.\n",
    "        if delta_w < 0:\n",
    "            w_old = self.network.input_2_output_connection.weights\n",
    "            w_new = w_old + self.lr * delta_w * (w_old - self.w_min)\n",
    "            self.network.input_2_output_connection.weights = w_new\n",
    "        \n",
    "        # Update the network weights if weight increment is positive.\n",
    "        elif delta_w > 0:\n",
    "            w_old = self.network.input_2_output_connection.weights\n",
    "            w_new = w_old + self.lr * delta_w * (self.w_max - w_old)\n",
    "            self.network.input_2_output_connection.weights = w_new\n",
    "    \n",
    "    def train_step(self, train_data_sample):\n",
    "        \"\"\"\n",
    "        Function to train the network for one training sample using the update function defined above. \n",
    "        Args:\n",
    "            train_data_sample (list): a sample from the training data\n",
    "            \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        input = train_data_sample[0]\n",
    "        output = train_data_sample[1]\n",
    "        for t in self.time:\n",
    "            if output[t] == 1:\n",
    "                for i in range(2):\n",
    "                    for t1 in self.sliding_window:\n",
    "                        if (0<= t + t1 < self.snn_timesteps) and (t1!=0) and (input[i][t+t1] == 1):\n",
    "                            self.update_weights(t1, i)\n",
    "    \n",
    "    def train(self, training_data):\n",
    "        \"\"\"\n",
    "        Function to train the network\n",
    "        \n",
    "        Args:\n",
    "            training_data (list): training data\n",
    "        \n",
    "        This function is complete. You do not need to do anything here. \n",
    "        \"\"\"\n",
    "        for ee in range(self.epochs):\n",
    "            for train_data_sample in training_data:\n",
    "                self.train_step(train_data_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Network Weights:\n",
      " [[0.96377245 0.99858174]]\n",
      "\n",
      "***TRAINING***\n",
      "Zero:\n",
      " [1 0 0 0 1 1 0 0 0 0]\n",
      "One:\n",
      " [0 1 1 1 0 0 1 0 1 0]\n",
      "Training Data:\n",
      " [[array([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0]])\n",
      "  array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0])]\n",
      " [array([[1, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
      "       [0, 1, 1, 1, 0, 0, 1, 0, 1, 0]])\n",
      "  array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0])]\n",
      " [array([[0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 1, 1, 0, 0, 0, 0]])\n",
      "  array([1, 0, 0, 0, 1, 1, 0, 0, 0, 0])]\n",
      " [array([[0, 1, 1, 1, 0, 0, 1, 0, 1, 0],\n",
      "       [0, 1, 1, 1, 0, 0, 1, 0, 1, 0]])\n",
      "  array([0, 1, 1, 1, 0, 0, 1, 0, 1, 0])]]\n",
      "\n",
      "***TESTING***\n",
      "Zero:\n",
      " [1 0 1 1 1 0 1 0 1 1]\n",
      "One:\n",
      " [1 0 1 0 0 1 1 0 0 0]\n",
      "Case 1: [0.7]\n",
      "Case 2: [0.3]\n",
      "Case 3: [0.3]\n",
      "Case 4: [0.4]\n",
      "Testing Data:\n",
      " [1, 0, 0, 0]\n",
      "\n",
      "Final Network Weights:\n",
      " [[0.27544411 0.27544411]]\n"
     ]
    }
   ],
   "source": [
    "# Define a variable for input dimension.\n",
    "input_dimension = 2\n",
    "\n",
    "# Define a variable for output dimension.\n",
    "output_dimension = 1\n",
    "\n",
    "# Define a variable for voltage decay.\n",
    "vdecay = 0.5\n",
    "\n",
    "# Define a variable for voltage threshold.\n",
    "vth = 0.5\n",
    "\n",
    "# Define a variable for snn timesteps.\n",
    "snn_timestep = 10\n",
    "\n",
    "# Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
    "\n",
    "# Print the initial network weights.\n",
    "print('\\nInitial Network Weights:\\n', input_2_output_weight)\n",
    "\n",
    "# Initialize a SNN using the arguments defined above.\n",
    "snn_4 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Get the training data for AND gate using the function defined in 2a.\n",
    "print('\\n***TRAINING***')\n",
    "train_data = genANDTrainData(snn_timestep)\n",
    "\n",
    "# Create an object of STDP class with appropriate arguments.\n",
    "stdp = STDP(snn_4, A_plus=0.6, A_minus=0.3, tau_plus=8, tau_minus=4, \n",
    "            lr=0.25, snn_timesteps=10, epochs=10, w_min=0, w_max=1)\n",
    "\n",
    "# Train the network using STDP.\n",
    "stdp.train(train_data)\n",
    "\n",
    "# Test the trained network and print the network output for all 4 cases.\n",
    "print('\\n***TESTING***')\n",
    "test_data = []\n",
    "\n",
    "zero = np.random.choice([1, 0], snn_timestep)\n",
    "print('Zero:\\n', zero)\n",
    "\n",
    "one = np.random.choice([1, 0], snn_timestep)\n",
    "print('One:\\n', one)\n",
    "\n",
    "case1 = np.array([zero, zero])\n",
    "case2 = np.array([zero, one])\n",
    "case3 = np.array([one, zero])\n",
    "case4 = np.array([one, one])\n",
    "\n",
    "# Case 1:\n",
    "output1 = snn_4(case1)\n",
    "print(\"Case 1:\", output1)\n",
    "if output1 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 2:\n",
    "output2 = snn_4(case2)\n",
    "print(\"Case 2:\", output2)\n",
    "if output2 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 3:\n",
    "output3 = snn_4(case3)\n",
    "print(\"Case 3:\", output3)\n",
    "if output3 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 4:\n",
    "output4 = snn_4(case4)\n",
    "print(\"Case 4:\", output4)\n",
    "if output4 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "print('Testing Data:\\n', test_data)\n",
    "\n",
    "# Print the final network weights.\n",
    "print(\"\\nFinal Network Weights:\\n\", snn_4.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: OR Gate\n",
    "Can you train the network with the same architecture in Q2-4 for learning the OR gate. You will need to create another function called genORTrainData. Then create an SNN and train it using STDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation of genORTrainData here. \n",
    "def genORTrainData(snn_timestep):\n",
    "    \"\"\" \n",
    "    Function to generate the training data for AND.\n",
    "        Args:\n",
    "            snn_timestep (int): timesteps for SNN simulation\n",
    "        Return:\n",
    "            train_data (list): list of tuples where each tuple comprises of numpy arrays of rate-coded inputs and output\n",
    "        \n",
    "        Write the expressions for encoding 0 and 1. Then append all 4 cases of OR gate to the list train_data.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list for train data.\n",
    "    train_data = []\n",
    "    \n",
    "    # Encode 0. Numpy random choice function might be useful here.\n",
    "    zero = np.random.choice([1, 0], snn_timestep)\n",
    "    print('Zero:\\n', zero)\n",
    "    \n",
    "    # Encode 1. Numpy random choice function might be useful here.\n",
    "    one = np.random.choice([1, 0], snn_timestep)\n",
    "    print('One:\\n', one)\n",
    "    \n",
    "    # Append all 4 cases of AND gate to train_data. Numpy stack operation might be useful here.\n",
    "    sub_case1 = np.array([zero, zero])\n",
    "    sub_case2 = np.array([zero, one])\n",
    "    sub_case3 = np.array([one, zero])\n",
    "    sub_case4 = np.array([one, one])\n",
    "    \n",
    "    case1 = np.array([sub_case1, zero]) # Change second param.\n",
    "    case2 = np.array([sub_case2, one])\n",
    "    case3 = np.array([sub_case3, one])\n",
    "    case4 = np.array([sub_case4, one])\n",
    "    \n",
    "    train_data = np.stack((case1, case2, case3, case4))\n",
    "    print('Training Data:\\n', train_data)\n",
    "    \n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Network Weights:\n",
      " [[0.58827154 0.5858281 ]]\n",
      "\n",
      "***TRAINING***\n",
      "Zero:\n",
      " [0 0 1 1 1 0 1 1 0 0]\n",
      "One:\n",
      " [1 1 0 0 0 1 1 1 0 1]\n",
      "Training Data:\n",
      " [[array([[0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
      "       [0, 0, 1, 1, 1, 0, 1, 1, 0, 0]])\n",
      "  array([0, 0, 1, 1, 1, 0, 1, 1, 0, 0])]\n",
      " [array([[0, 0, 1, 1, 1, 0, 1, 1, 0, 0],\n",
      "       [1, 1, 0, 0, 0, 1, 1, 1, 0, 1]])\n",
      "  array([1, 1, 0, 0, 0, 1, 1, 1, 0, 1])]\n",
      " [array([[1, 1, 0, 0, 0, 1, 1, 1, 0, 1],\n",
      "       [0, 0, 1, 1, 1, 0, 1, 1, 0, 0]])\n",
      "  array([1, 1, 0, 0, 0, 1, 1, 1, 0, 1])]\n",
      " [array([[1, 1, 0, 0, 0, 1, 1, 1, 0, 1],\n",
      "       [1, 1, 0, 0, 0, 1, 1, 1, 0, 1]])\n",
      "  array([1, 1, 0, 0, 0, 1, 1, 1, 0, 1])]]\n",
      "\n",
      "***TESTING***\n",
      "Zero:\n",
      " [0 1 1 1 0 0 0 0 1 0]\n",
      "One:\n",
      " [1 0 1 0 1 0 0 1 1 1]\n",
      "Case 1: [0.1]\n",
      "Case 2: [0.]\n",
      "Case 3: [0.1]\n",
      "Case 4: [0.2]\n",
      "Testing Data:\n",
      " [0, 0, 0, 0]\n",
      "\n",
      "Final Network Weights:\n",
      " [[0.17177952 0.17177952]]\n"
     ]
    }
   ],
   "source": [
    "# Train the network for OR gate here using the implementation from 4c.\n",
    "# Define a variable for input dimension.\n",
    "input_dimension = 2\n",
    "\n",
    "# Define a variable for output dimension.\n",
    "output_dimension = 1\n",
    "\n",
    "# Define a variable for voltage decay.\n",
    "vdecay = 0.5\n",
    "\n",
    "# Define a variable for voltage threshold.\n",
    "vth = 0.5\n",
    "\n",
    "# Define a variable for snn timesteps.\n",
    "snn_timestep = 10\n",
    "\n",
    "# Initialize randomly the weights from input to output. Numpy random rand function might be useful here.\n",
    "input_2_output_weight = np.random.rand(output_dimension, input_dimension)\n",
    "\n",
    "# Print the initial network weights.\n",
    "print('\\nInitial Network Weights:\\n', input_2_output_weight)\n",
    "\n",
    "# Initialize a SNN using the arguments defined above.\n",
    "snn_5 = SNN(input_2_output_weight, input_dimension, output_dimension, vdecay, vth, snn_timestep)\n",
    "\n",
    "# Get the training data for AND gate using the function defined in 2a.\n",
    "print('\\n***TRAINING***')\n",
    "train_data = genORTrainData(snn_timestep)\n",
    "\n",
    "# Create an object of STDP class with appropriate arguments.\n",
    "stdp = STDP(snn_5, A_plus=0.6, A_minus=0.3, tau_plus=8, tau_minus=4, \n",
    "            lr=0.25, snn_timesteps=10, epochs=10, w_min=0, w_max=1)\n",
    "\n",
    "# Train the network using STDP.\n",
    "stdp.train(train_data)\n",
    "\n",
    "# Test the trained network and print the network output for all 4 cases.\n",
    "print('\\n***TESTING***')\n",
    "test_data = []\n",
    "\n",
    "zero = np.random.choice([1, 0], snn_timestep)\n",
    "print('Zero:\\n', zero)\n",
    "\n",
    "one = np.random.choice([1, 0], snn_timestep)\n",
    "print('One:\\n', one)\n",
    "\n",
    "case1 = np.array([zero, zero])\n",
    "case2 = np.array([zero, one])\n",
    "case3 = np.array([one, zero])\n",
    "case4 = np.array([one, one])\n",
    "\n",
    "# Case 1:\n",
    "output1 = snn_5(case1)\n",
    "print(\"Case 1:\", output1)\n",
    "if output1 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 2:\n",
    "output2 = snn_5(case2)\n",
    "print(\"Case 2:\", output2)\n",
    "if output2 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 3:\n",
    "output3 = snn_5(case3)\n",
    "print(\"Case 3:\", output3)\n",
    "if output3 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "# Case 4:\n",
    "output4 = snn_5(case4)\n",
    "print(\"Case 4:\", output4)\n",
    "if output4 > vth:\n",
    "    test_data.append(1)\n",
    "else:\n",
    "    test_data.append(0)\n",
    "\n",
    "print('Testing Data:\\n', test_data)\n",
    "\n",
    "# Print the final network weights.\n",
    "print(\"\\nFinal Network Weights:\\n\", snn_5.input_2_output_connection.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
